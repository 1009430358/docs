# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2008-2016
# This file is distributed under the same license as the mongodb-manual package.
# FIRST AUTHOR <EMAIL@ADDRESS>, YEAR.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: mongodb-manual 3.2\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2016-05-31 19:15-0400\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"

#: ../source/faq/diagnostics.txt:5
# c4bd9ec0b99041b5bc1a37664f133f0e
msgid "FAQ: MongoDB Diagnostics"
msgstr ""

#: ../source/faq/diagnostics.txt:0
# 984e695468cf4a5f83eb62bb7388452a
msgid "On this page"
msgstr ""

#: ../source/faq/diagnostics.txt:15
# 36e46b0fd22d49c7aa253a6d871b1092
msgid "This document provides answers to common diagnostic questions and issues."
msgstr ""

#: ../source/faq/diagnostics.txt:18
# f00606efad7c4b98b4e4f3da888b6fd7
msgid "If you don't find the answer you're looking for, check the :doc:`complete list of FAQs </faq>` or post your question to the `MongoDB User Mailing List <https://groups.google.com/forum/?fromgroups#!forum/mongodb-user>`_."
msgstr ""

#: ../source/faq/diagnostics.txt:23
# 85efbd485d0042c6b349fcc01d525e3c
msgid "Where can I find information about a ``mongod`` process that stopped running unexpectedly?"
msgstr ""

#: ../source/faq/diagnostics.txt:25
# 36a7cb881e7b4f8683ce36263ee14557
msgid "If :program:`mongod` shuts down unexpectedly on a UNIX or UNIX-based platform, and if :program:`mongod` fails to log a shutdown or error message, then check your system logs for messages pertaining to MongoDB. For example, for logs located in ``/var/log/messages``, use the following commands:"
msgstr ""

#: ../source/faq/diagnostics.txt:39
# beaa76834a5040369ebfa29623f6511c
msgid "Does TCP ``keepalive`` time affect MongoDB Deployments?"
msgstr ""

#: ../source/faq/diagnostics.txt:41
# 28080157f73e4daf9a9802e70cea6459
msgid "If you experience socket errors between clients and servers or between members of a sharded cluster or replica set that do not have other reasonable causes, check the TCP keepalive value (e.g. on Linux systems store, the ``tcp_keepalive_time`` value). A common keepalive period is ``7200`` seconds (2 hours); however, different distributions and OS X may have different settings."
msgstr ""

#: ../source/faq/diagnostics.txt:48
# 8d30b07585134ed0a36e491ec732669c
msgid "For MongoDB, you will have better results with shorter keepalive periods, on the order of ``120`` seconds (two minutes)."
msgstr ""

#: ../source/faq/diagnostics.txt:51
# c9fe29f4d25241bcbc2e9b1d47b655d7
msgid "If your MongoDB deployment experiences keepalive-related issues, you must alter the keep alive value on *all* machines hosting MongoDB processes. This includes all machines hosting :program:`mongos` or :program:`mongod` servers and all machines hosting client processes that connect to MongoDB."
msgstr ""

#: ../source/faq/diagnostics.txt:59
# aa62ca47d7d243e7bc1fadeff2ccbbd9
msgid "For non-Linux systems, values greater than or equal to 600 seconds (10 minutes) will be ignored by :program:`mongod` and :program:`mongos`. For Linux, values greater than 300 seconds (5 minutes) will be overridden on the :program:`mongod` and :program:`mongos` sockets with a maximum of 300 seconds."
msgstr ""

#: ../source/includes/fact-tcp-keepalive-linux.rst:1
# a87b48a5a98f4646a3fff0f6a6d226b2
msgid "**On Linux systems**:"
msgstr ""

#: ../source/includes/fact-tcp-keepalive-linux.rst:3
# aaf6f6250db446a18c58cb6b78de6249
msgid "To view the keep alive setting, you can use one of the following commands:"
msgstr ""

#: ../source/includes/fact-tcp-keepalive-linux.rst:10
#: ../source/includes/fact-tcp-keepalive-linux.rst:25
# 4324d46753f84a02bbc3c44bcfac0a39
# 204a5fe6f508407b8335187bf2dc1fe4
msgid "Or:"
msgstr ""

#: ../source/includes/fact-tcp-keepalive-linux.rst:16
# 4c75f8329b754ffa803abd9f2f4d55e2
msgid "The value is measured in seconds."
msgstr ""

#: ../source/includes/fact-tcp-keepalive-linux.rst:18
# ef95fa0fed3c44a58629d9feb03ee69b
msgid "To change the ``tcp_keepalive_time`` value, you can use one of the following command:"
msgstr ""

#: ../source/includes/fact-tcp-keepalive-linux.rst:31
# 28fa150e82c548128c658e526f0a6f60
msgid "These operations do not persist across system reboots. To persist the setting, add the following line to ``/etc/sysctl.conf``:"
msgstr ""

#: ../source/includes/fact-tcp-keepalive-linux.rst:38
# 7d6358da47424302812a7f58c10f7a8d
msgid "On Linux, :program:`mongod` and :program:`mongos` processes limit the keepalive to a maximum of 300 seconds (5 minutes) on their own sockets by overriding keepalive values greater than 5 minutes."
msgstr ""

#: ../source/includes/fact-tcp-keepalive-osx.rst:1
# 77f1a8746f504641a97a36855d42a786
msgid "**For OS X systems**:"
msgstr ""

#: ../source/includes/fact-tcp-keepalive-osx.rst:3
#: ../source/includes/fact-tcp-keepalive-windows.rst:3
# efc3bced280146a282f42232642846f4
# ea9e16b734df4c60b301ef931982fb8f
msgid "To view the keep alive setting, issue the following command:"
msgstr ""

#: ../source/includes/fact-tcp-keepalive-osx.rst:9
# 0a35f553f5024224b1596fdfe656cc66
msgid "To change the ``net.inet.tcp.keepinit`` value, you can use the following command:"
msgstr ""

#: ../source/includes/fact-tcp-keepalive-osx.rst:16
# 5eb7f77f3e534c41bb80af6438d8c2fb
msgid "The above method for setting the TCP keepalive is not persistent; you will need to reset the value each time you reboot or restart a system. See your operating systemâ€™s documentation for instructions on setting the TCP keepalive value persistently."
msgstr ""

#: ../source/includes/fact-tcp-keepalive-windows.rst:1
# b6e9b4cd146b42a6a869fe5fbc12c5f1
msgid "**For Windows systems**:"
msgstr ""

#: ../source/includes/fact-tcp-keepalive-windows.rst:9
# 027bc8d8952a4ac6a41d069be1a2b45b
msgid "The registry value is not present by default. The system default, used if the value is absent, is 7200000 *milliseconds* or ``0x6ddd00`` in hexadecimal."
msgstr ""

#: ../source/includes/fact-tcp-keepalive-windows.rst:13
# ca5aa22e4481423486932326019566d3
msgid "To change the ``KeepAliveTime`` value, use the following command in an Administrator :guilabel:`Command Prompt`, where ``<value>`` is expressed in hexadecimal (e.g. ``0x0124c0`` is 120000):"
msgstr ""

#: ../source/includes/fact-tcp-keepalive-windows.rst:21
# 4ad2c38e831a41629ceff02575363dc7
msgid "Windows users should consider the `Windows Server Technet Article on KeepAliveTime <https://technet.microsoft.com/en-us/library/cc957549.aspx>`_ for more information on setting keep alive for MongoDB deployments on Windows systems."
msgstr ""

#: ../source/faq/diagnostics.txt:71
# f0e121c3c2cf41a6b9b9768721861916
msgid "You will need to restart :program:`mongod` and :program:`mongos` servers for new system-wide keepalive settings to take effect."
msgstr ""

#: ../source/faq/diagnostics.txt:75
# 9a94c63c15f7448ebbb07845b8aa2f9e
msgid "Why does MongoDB log so many \"Connection Accepted\" events?"
msgstr ""

#: ../source/faq/diagnostics.txt:77
# e661f400b5724fb5934290a01c7c83db
msgid "If you see a very large number connection and re-connection messages in your MongoDB log, then clients are frequently connecting and disconnecting to the MongoDB server. This is normal behavior for applications that do not use request pooling, such as CGI. Consider using FastCGI, an Apache Module, or some other kind of persistent application server to decrease the connection overhead."
msgstr ""

#: ../source/faq/diagnostics.txt:84
# 2e6698a529ac4feba562b31e83d6ca4d
msgid "If these connections do not impact your performance you can use the run-time :setting:`~systemLog.quiet` option or the command-line option :option:`--quiet <mongod --quiet>` to suppress these messages from the log."
msgstr ""

#: ../source/faq/diagnostics.txt:90
# 6221d01285f448b1a1bffdc01664671b
msgid "What tools are available for monitoring MongoDB?"
msgstr ""

#: ../source/faq/diagnostics.txt:92
# 870d8ce8a98b47d0a12f32a081f62b0a
msgid "The |mms-home| and :products:`Ops Manager, an on-premise solution available in MongoDB Enterprise Advanced </mongodb-enterprise-advanced?jmp=docs>` include monitoring functionality, which collects data from running MongoDB deployments and provides visualization and alerts based on that data."
msgstr ""

#: ../source/faq/diagnostics.txt:98
# 7fc90f97f4c64135afed4a96f2fc0866
msgid "For more information, see also the |mms-docs| and :opsmgr:`Ops Manager documentation </application>`."
msgstr ""

#: ../source/faq/diagnostics.txt:101
# 6c368606fc1441dea7b51d33f296ca8e
msgid "A full list of third-party tools is available as part of the :doc:`/administration/monitoring/` documentation."
msgstr ""

#: ../source/faq/diagnostics.txt:109
# 017bbca59f644b54a99599a48d3a5250
msgid "Memory Diagnostics for the MMAPv1 Storage Engine"
msgstr ""

#: ../source/faq/diagnostics.txt:112
# e711c80615204777a9b908ea2cd3ce49
msgid "Do I need to configure swap space?"
msgstr ""

#: ../source/faq/diagnostics.txt:114
# 02709d512d564f2ea59d0c022aa415ec
msgid "Always configure systems to have swap space. Without swap, your system may not be reliant in some situations with extreme memory constraints, memory leaks, or multiple programs using the same memory.  Think of the swap space as something like a steam release valve that allows the system to release extra pressure without affecting the overall functioning of the system."
msgstr ""

#: ../source/faq/diagnostics.txt:121
# 33b86d9d11f344b8b20f6c55e2faae57
msgid "Nevertheless, systems running MongoDB *do not* need swap for routine operation. Database files are :ref:`memory-mapped <faq-storage-memory-mapped-files>` and should constitute most of your MongoDB memory use. Therefore, it is unlikely that :program:`mongod` will ever use any swap space in normal operation. The operating system will release memory from the memory mapped files without needing swap and MongoDB can write data to the data files without needing the swap system."
msgstr ""

#: ../source/faq/diagnostics.txt:133
# 66fcd2e88164442996a11919de61fcff
msgid "What is a \"working set\"?"
msgstr ""

#: ../source/faq/diagnostics.txt:135
# 35d9802fcfcb48c2b7ffaf00647fc32d
msgid "The *working set* is the portion of your data that clients access most often."
msgstr ""

#: ../source/faq/diagnostics.txt:139
#: ../source/faq/diagnostics.txt:214
# 40f30e69cf094f2db6f082ec3e07e768
# 586e4c277a154ca79159a41c3a123f2e
msgid "Must my working set size fit RAM?"
msgstr ""

#: ../source/faq/diagnostics.txt:141
# 547c74f926bc44fc88e7565c8c495128
msgid "Your working set should stay in memory to achieve good performance. Otherwise many random disk IO's will occur, and unless you are using SSD, this can be quite slow."
msgstr ""

#: ../source/faq/diagnostics.txt:145
# cda423d30eb1404690bfb1b1a172f6d3
msgid "One area to watch specifically in managing the size of your working set is index access patterns. If you are inserting into indexes at random locations (as would happen with id's that are randomly generated by hashes), you will continually be updating the whole index. If instead you are able to create your id's in approximately ascending order (for example, day concatenated with a random id), all the updates will occur at the right side of the b-tree and the working set size for index pages will be much smaller."
msgstr ""

#: ../source/faq/diagnostics.txt:154
# fe964cbc4cd3481fa3d1f68f0979825f
msgid "It is fine if databases and thus virtual size are much larger than RAM."
msgstr ""

#: ../source/faq/diagnostics.txt:157
#: ../source/faq/diagnostics.txt:221
# fd45637e6e784034af5c0bd895b296a5
# 78da405355ae4de3b17d8b3ce4053a36
msgid "How do I calculate how much RAM I need for my application?"
msgstr ""

#: ../source/faq/diagnostics.txt:161
# 1092797ac81e4a369390d0ac73ee1af9
msgid "The amount of RAM you need depends on several factors, including but not limited to:"
msgstr ""

#: ../source/faq/diagnostics.txt:164
# d0e58f1f5ec44527a93f3c920f77f5f3
msgid "The relationship between :doc:`database storage </faq/storage>` and working set."
msgstr ""

#: ../source/faq/diagnostics.txt:166
# 193d4e0a572643efb55ac497076a0215
msgid "The operating system's cache strategy for LRU (Least Recently Used)"
msgstr ""

#: ../source/faq/diagnostics.txt:168
# 42aae21cf26f4f3bb06742c075ff5ee0
msgid "The impact of :doc:`journaling </core/journaling>`"
msgstr ""

#: ../source/faq/diagnostics.txt:170
# 1835edf101e843dd924ba22f8d25ce7f
msgid "The number or rate of page faults and other |MMS| gauges to detect when you need more RAM"
msgstr ""

#: ../source/faq/diagnostics.txt:173
# c101415b39544cc997c6f479838712d3
msgid "Each database connection thread will need up to 1 MB of RAM."
msgstr ""

#: ../source/faq/diagnostics.txt:175
# ecdd5990ffc5467b9227e89d1f92371a
msgid "MongoDB defers to the operating system when loading data into memory from disk. It simply :ref:`memory maps <faq-storage-memory-mapped-files>` all its data files and relies on the operating system to cache data. The OS typically evicts the least-recently-used data from RAM when it runs low on memory. For example if clients access  indexes more frequently than documents, then indexes will more likely stay in RAM, but it depends on your particular usage."
msgstr ""

#: ../source/faq/diagnostics.txt:183
# 6871e59f38da4381bc9858ca38036c83
msgid "To calculate how much RAM you need, you must calculate your working set size, or the portion of your data that clients use most often. This depends on your access patterns, what indexes you have, and the size of your documents. Because MongoDB uses a thread per connection model, each database connection also will need up to 1 MB of RAM, whether active or idle."
msgstr ""

#: ../source/faq/diagnostics.txt:189
# a51f741e99e04dbba2f093014c37dc24
msgid "If page faults are infrequent, your working set fits in RAM. If fault rates rise higher than that, you risk performance degradation. This is less critical with SSD drives than with spinning disks."
msgstr ""

#: ../source/faq/diagnostics.txt:195
# c567406d346746e28c82dfa91b8d1753
msgid "How do I read memory statistics in the UNIX ``top`` command"
msgstr ""

#: ../source/faq/diagnostics.txt:197
# 3493e505df8b48eb82b0b92af5a5c69e
msgid "Because :program:`mongod` uses :ref:`memory-mapped files <faq-storage-memory-mapped-files>`, the memory statistics in ``top`` require interpretation in a special way. On a large database, ``VSIZE`` (virtual bytes) tends to be the size of the entire database. If the :program:`mongod` doesn't have other processes running, ``RSIZE`` (resident bytes) is the total memory of the machine, as this counts file system cache contents."
msgstr ""

#: ../source/faq/diagnostics.txt:205
# 574f075ed50643f3b99a718c7523ea93
msgid "For Linux systems, use the ``vmstat`` command to help determine how the system uses memory. On OS X systems use ``vm_stat``."
msgstr ""

#: ../source/faq/diagnostics.txt:211
# dd2f5f7bd632489fbac24a9418e5da15
msgid "Memory Diagnostics for the WiredTiger Storage Engine"
msgstr ""

#: ../source/faq/diagnostics.txt:216
# acd1c980b8a442a7ab863e5559e48098
msgid "No."
msgstr ""

#: ../source/includes/extracts/wt-cache-eviction.rst:1
# 02de7b6b88a54a46bff9816d7b37bba4
msgid "If the cache does not have enough space to load additional data, WiredTiger evicts pages from the cache to free up space."
msgstr ""

#: ../source/includes/extracts/wt-cache-additional-constraints-mongod-config.rst:3
#: ../source/includes/extracts/wt-cache-additional-constraints-mongod-config.rst:3
# 25afe77aee6040f6bea7bd019ec9a19d
# af02aa4f370641e38674eaf10385017f
msgid "The :setting:`storage.wiredTiger.engineConfig.cacheSizeGB` only limits the size of the WiredTiger cache, not the total amount of memory used by :program:`mongod`. The WiredTiger cache is only one component of the RAM used by MongoDB. MongoDB also automatically uses all free memory on the machine via the filesystem cache (data in the filesystem cache is compressed)."
msgstr ""

#: ../source/includes/extracts/wt-cache-additional-constraints-mongod-config.rst:10
#: ../source/includes/extracts/wt-cache-additional-constraints-mongod-config.rst:10
# c2050b94a747459d82c06a669e54c758
# 9f9ffd76edd244fb8db70375934fe57b
msgid "In addition, the operating system will use any free RAM to buffer filesystem blocks."
msgstr ""

#: ../source/includes/extracts/wt-cache-additional-constraints-mongod-config.rst:13
#: ../source/includes/extracts/wt-cache-additional-constraints-mongod-config.rst:13
# e6509548dd8e47b98cea6c1da2533300
# 8b313deb128646e49a5d1fc5cb9b244c
msgid "To accommodate the additional consumers of RAM, you may have to decrease WiredTiger cache size."
msgstr ""

#: ../source/includes/extracts/wt-cache-additional-constraints-mongod-config.rst:16
#: ../source/includes/extracts/wt-cache-additional-constraints-mongod-config.rst:16
# d161c2d7b8f84e66b2347b9a37b3a955
# 6ff3cbc74c384aaebcd5ffc3709cda22
msgid "The default WiredTiger cache size value assumes that there is a single :program:`mongod` instance per machine. If a single machine contains multiple MongoDB instances, then you should decrease the setting to accommodate the other :program:`mongod` instances."
msgstr ""

#: ../source/includes/extracts/wt-cache-additional-constraints-mongod-config.rst:22
#: ../source/includes/extracts/wt-cache-additional-constraints-mongod-config.rst:22
# 7cb7590153a44e2cbb1a754442db1d23
# 3183cb42ba13444f97c1a623da50e2b0
msgid "If you run :program:`mongod` in a container (e.g. ``lxc``, ``cgroups``, Docker, etc.) that does *not* have access to all of the RAM available in a system, you must set :setting:`storage.wiredTiger.engineConfig.cacheSizeGB` to a value less than the amount of RAM available in the container. The exact amount depends on the other processes running in the container."
msgstr ""

#: ../source/includes/extracts/wt-cache-size.rst:4
# 917703d9ecff4c5f8a43a667e9f47e3f
msgid "To see statistics on the cache and eviction, use the :dbcommand:`serverStatus` command. The :serverstatus:`wiredTiger.cache` field holds the information on the cache and eviction."
msgstr ""

#: ../source/includes/extracts/wt-cache-size.rst:49
# 5d254ff2dab94c2c9926032d19abb520
msgid "For an explanation of some key cache and eviction statistics, such as :serverstatus:`wiredTiger.cache.bytes currently in the cache` and :serverstatus:`wiredTiger.cache.tracked dirty bytes in the cache`, see :serverstatus:`wiredTiger.cache`."
msgstr ""

#: ../source/includes/extracts/wt-cache-setting.rst:1
#: ../source/includes/extracts/wt-cache-setting.rst:1
# 15fed4e7680d44d18f4f540c63326d60
# 34f764a8fc4c4097b15e086b35b72042
msgid "To adjust the size of the WiredTiger cache, see :setting:`storage.wiredTiger.engineConfig.cacheSizeGB` and :option:`--wiredTigerCacheSizeGB`. Avoid increasing the WiredTiger cache size above its default value."
msgstr ""

#: ../source/includes/extracts/wt-cache-default-setting.rst:1
# dcde56f1dbb14b27ac6de18cc97347f2
msgid "With WiredTiger, MongoDB utilizes both the WiredTiger cache and the filesystem cache."
msgstr ""

#: ../source/includes/extracts/wt-cache-default-setting.rst:6
# 8c2860e903cc4c8692ec0cee2def563b
msgid "Starting in MongoDB 3.2, the WiredTiger cache, by default, will use the larger of either:"
msgstr ""

#: ../source/includes/extracts/wt-cache-default-setting.rst:9
# 6f2ed89d8a664f4a9c31cd8ab83df762
msgid "60% of RAM minus 1 GB, or"
msgstr ""

#: ../source/includes/extracts/wt-cache-default-setting.rst:11
# 97e60999f1e449cda89006d819343ee4
msgid "1 GB."
msgstr ""

#: ../source/includes/extracts/wt-cache-default-setting.rst:13
# a798ebfd131d4e4296320c9a6096fbc1
msgid "For systems with up to 10 GB of RAM, the new default setting is less than or equal to the 3.0 default setting (For MongoDB 3.0, the WiredTiger cache uses either 1 GB or half of the installed physical RAM, whichever is larger)."
msgstr ""

#: ../source/includes/extracts/wt-cache-default-setting.rst:18
# 65177de3c3e54e62be352356dc505793
msgid "For systems with more than 10 GB of RAM, the new default setting is greater than the 3.0 setting."
msgstr ""

#: ../source/includes/extracts/wt-cache-default-setting.rst:21
# dff7d1d1d13647feafbbe1d0e0c1b8f6
msgid "Via the filesystem cache, MongoDB automatically uses all free memory that is not used by the WiredTiger cache or by other processes. Data in the filesystem cache is compressed."
msgstr ""

#: ../source/includes/extracts/wt-configure-cache.rst:7
# b590e365c5904edc9d1e95dc7e9806b8
msgid "To view statistics on the cache and eviction rate, see the :serverstatus:`wiredTiger.cache` field returned from the :dbcommand:`serverStatus` command."
msgstr ""

#: ../source/faq/diagnostics.txt:226
# d42b3fb2a38c4792bb386f8d4547be75
msgid "Sharded Cluster Diagnostics"
msgstr ""

#: ../source/faq/diagnostics.txt:228
# b7dfb219df89487685d0727fd1eea445
msgid "The two most important factors in maintaining a successful sharded cluster are:"
msgstr ""

#: ../source/faq/diagnostics.txt:230
# d34a357fbcbe48c197fd7836f256c4ae
msgid ":ref:`choosing an appropriate shard key <sharding-internals-shard-keys>` and"
msgstr ""

#: ../source/faq/diagnostics.txt:232
# da046c2a1cb041a4ad265df5308e8f6c
msgid ":ref:`sufficient capacity to support current and future operations <sharding-capacity-planning>`."
msgstr ""

#: ../source/faq/diagnostics.txt:235
# 98f2065cfc684f448fb3f597388118ec
msgid "You can prevent most issues encountered with sharding by ensuring that you choose the best possible :term:`shard key` for your deployment and ensure that you are always adding additional capacity to your cluster well before the current resources become saturated. Continue reading for specific issues you may encounter in a production environment."
msgstr ""

#: ../source/faq/diagnostics.txt:244
# 938ba30bfb6f4797a4bef6e9303961f3
msgid "In a new sharded cluster, why does all data remains on one shard?"
msgstr ""

#: ../source/faq/diagnostics.txt:246
# 6cddeb1b148e40f7818622c14ec8a65b
msgid "Your cluster must have sufficient data for sharding to make sense. Sharding works by migrating chunks between the shards until each shard has roughly the same number of chunks."
msgstr ""

#: ../source/faq/diagnostics.txt:250
# 8e346fc101414e5284d6f88d76c07302
msgid "The default chunk size is 64 megabytes. MongoDB will not begin migrations until the imbalance of chunks in the cluster exceeds the :ref:`migration threshold <sharding-migration-thresholds>`. While the default chunk size is configurable with the :setting:`~sharding.chunkSize` setting, these behaviors help prevent unnecessary chunk migrations, which can degrade the performance of your cluster as a whole."
msgstr ""

#: ../source/faq/diagnostics.txt:257
# 15322745eda7442b8ada5eedbd72fe53
msgid "If you have just deployed a sharded cluster, make sure that you have enough data to make sharding effective. If you do not have sufficient data to create more than eight 64 megabyte chunks, then all data will remain on one shard. Either lower the :ref:`chunk size <sharding-chunk-size>` setting, or add more data to the cluster."
msgstr ""

#: ../source/faq/diagnostics.txt:263
# 460c8b64df1c4d2da03a7843d4a6cb04
msgid "As a related problem, the system will split chunks only on inserts or updates, which means that if you configure sharding and do not continue to issue insert and update operations, the database will not create any chunks. You can either wait until your application inserts data *or* :doc:`split chunks manually </tutorial/split-chunks-in-sharded-cluster>`."
msgstr ""

#: ../source/faq/diagnostics.txt:269
# f9c130fb0e834530a6a72aa37a9be765
msgid "Finally, if your shard key has a low :ref:`cardinality <sharding-shard-key-cardinality>`, MongoDB may not be able to create sufficient splits among the data."
msgstr ""

#: ../source/faq/diagnostics.txt:274
# d7306565af2d4a0ea50067ab9e9fdb27
msgid "Why would one shard receive a disproportion amount of traffic in a sharded cluster?"
msgstr ""

#: ../source/faq/diagnostics.txt:276
# a079a0baf1e149fc83f7eb977b58308a
msgid "In some situations, a single shard or a subset of the cluster will receive a disproportionate portion of the traffic and workload. In almost all cases this is the result of a shard key that does not effectively allow :ref:`write scaling <sharding-shard-key-write-scaling>`."
msgstr ""

#: ../source/faq/diagnostics.txt:281
# 040795b1c9f647ba9296cc17868e15aa
msgid "It's also possible that you have \"hot chunks.\" In this case, you may be able to solve the problem by splitting and then migrating parts of these chunks."
msgstr ""

#: ../source/faq/diagnostics.txt:285
# 59c2c0765c7c4f4c89ca82ba7878f7ea
msgid "In the worst case, you may have to consider re-sharding your data and :ref:`choosing a different shard key <sharding-internals-choose-shard-key>` to correct this pattern."
msgstr ""

#: ../source/faq/diagnostics.txt:290
# b34a885c8c4d49b9a597dcf32987a60e
msgid "What can prevent a sharded cluster from balancing?"
msgstr ""

#: ../source/faq/diagnostics.txt:292
# 78b4bc32780441e58cda4feaf94a559f
msgid "If you have just deployed your sharded cluster, you may want to consider the :ref:`troubleshooting suggestions for a new cluster where data remains on a single shard <sharding-troubleshooting-not-splitting>`."
msgstr ""

#: ../source/faq/diagnostics.txt:296
# 710298dbe38b49c8ba50928f329da0c5
msgid "If the cluster was initially balanced, but later developed an uneven distribution of data, consider the following possible causes:"
msgstr ""

#: ../source/faq/diagnostics.txt:299
# 1ab2a0fc819e4b29b276fa501eadab1f
msgid "You have deleted or removed a significant amount of data from the cluster. If you have added additional data, it may have a different distribution with regards to its shard key."
msgstr ""

#: ../source/faq/diagnostics.txt:303
# 5f4e466120df4716a6eb2cb1ba1ae4ce
msgid "Your :term:`shard key` has low :ref:`cardinality <sharding-shard-key-cardinality>` and MongoDB cannot split the chunks any further."
msgstr ""

#: ../source/faq/diagnostics.txt:306
# 0c58b94d384e46639b0e4d7b88bb7a5e
msgid "Your data set is growing faster than the balancer can distribute data around the cluster. This is uncommon and typically is the result of:"
msgstr ""

#: ../source/faq/diagnostics.txt:310
# 1f56b4aa2d6e4a16a5a821127d6f7c48
msgid "a :ref:`balancing window <sharding-schedule-balancing-window>` that is too short, given the rate of data growth."
msgstr ""

#: ../source/faq/diagnostics.txt:313
# 7584aeb17fab4a7698d291394699fc36
msgid "an uneven distribution of :ref:`write operations <sharding-shard-key-write-scaling>` that requires more data migration. You may have to choose a different shard key to resolve this issue."
msgstr ""

#: ../source/faq/diagnostics.txt:318
# 65fb058ed6994bc08771081e580148c3
msgid "poor network connectivity between shards, which may lead to chunk migrations that take too long to complete. Investigate your network configuration and interconnections between shards."
msgstr ""

#: ../source/faq/diagnostics.txt:323
# 8bd14520193d456fa5365fe61bc9ed6f
msgid "Why do chunk migrations affect sharded cluster performance?"
msgstr ""

#: ../source/faq/diagnostics.txt:325
# c0c1eacb61c5419a8d892dd636fcff4e
msgid "If migrations impact your cluster or application's performance, consider the following options, depending on the nature of the impact:"
msgstr ""

#: ../source/faq/diagnostics.txt:328
# cdf8b6e360224da1983ae98c89fa9d54
msgid "If migrations only interrupt your clusters sporadically, you can limit the :ref:`balancing window <sharding-schedule-balancing-window>` to prevent balancing activity during peak hours. Ensure that there is enough time remaining to keep the data from becoming out of balance again."
msgstr ""

#: ../source/faq/diagnostics.txt:334
# e11f482972ea4098b6d94e56ae903808
msgid "If the balancer is always migrating chunks to the detriment of overall cluster performance:"
msgstr ""

#: ../source/faq/diagnostics.txt:337
# 5725b5a273d445939f8982c27b72989d
msgid "You may want to attempt :doc:`decreasing the chunk size </tutorial/modify-chunk-size-in-sharded-cluster>` to limit the size of the migration."
msgstr ""

#: ../source/faq/diagnostics.txt:340
# 95ac91aace0b48ff91694e48674beee6
msgid "Your cluster may be over capacity, and you may want to attempt to :ref:`add one or two shards <sharding-procedure-add-shard>` to the cluster to distribute load."
msgstr ""

#: ../source/faq/diagnostics.txt:344
# b3986ec5db4c49649739aeb20a970d97
msgid "It's also possible that your shard key causes your application to direct all writes to a single shard. This kind of activity pattern can require the balancer to migrate most data soon after writing it. Consider redeploying your cluster  with a shard key that provides better :ref:`write scaling <sharding-shard-key-write-scaling>`."
msgstr ""

